\documentclass[twocolumn]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{url}
\usepackage{color}
\usepackage{float}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[numbers]{natbib}
%\usepackage{ctex}
%\geometry{top=1in, bottom=1in, left=1.25in, right=1.25in}
% 设置页面边距
\geometry{top=1in, bottom=1in, left=1in, right=1in}

% 调整列间距
\setlength{\columnsep}{30pt} % 设置两列之间的间距为20pt（可以根据需要调整）


\title{Learning-guided Monte Carlo Methods for Efficient Sampling}
\author{Liu Yu}

%\\ Department of Systems Engineering and Engineering Management \\  Chinese University of Hong Kong

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This research intends to propose the development of Learning-guided Monte Carlo Methods (LgMC), an innovative approach that integrates deep learning techniques with traditional Markov Chain Monte Carlo (MCMC) methods. The primary goal of this approach is to enhance the efficiency of Monte Carlo methods in handling high-dimensional and complex probabilistic inference challenges. We propose an improved example through applying a guiding function to the traditional Metropolis-Hastings algorithm. The proposed method focuses on making the sampling more biased towards high probability regions of the target distribution. This is not a direct transformation of distribution but a intuitive strategy to improve sampling efficiency. The research aims to significantly reduce the computational burden and 
accelerate the convergence of MCMC algorithm.

%enhance the accuracy of MC-based inference processes in fields such as Bayesian inference, probabilistic graphical models, and reinforcement learning, where efficient and accurate probabilistic reasoning is crucial.
\end{abstract}

\section{Overview}

\subsection{Background}
 Markov Chain Monte Carlo (MCMC), have long been foundational tools in numerical approximation and probabilistic inference . MCMC encompasses a variety of algorithms, with the Metropolis-Hastings (M-H) algorithm being one of the most notable \cite{metropolis1953equation}. The M-H algorithm generates samples from a probability distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution \cite{robert2004monte}. Despite its foundational role, traditional MCMC methods like M-H often face challenges, particularly in high-dimensional spaces \cite{gelman1995efficient}, complex distributions, or when dealing with sparse data.

To address some of the limitations of MCMC, researchers have developed several advanced techniques. Methods aimed at accelerating convergence focus on modifications in proposal distributions or utilizing Hamiltonian dynamics, as seen in Hamiltonian Monte Carlo (HMC) \cite{neal2011mcmc}, to enhance the exploration of the parameter space and reduce the autocorrelation of the samples. However, even HMC struggle to transition between metastable basins leading to either extremely long correlation times along the chains and few effective independent samples, or even failure to equilibrate at all. 

Moreover, Automatic Hyperparameter Tuning has become an increasingly important aspect of MCMC methods to improve performance without requiring extensive manual tuning \cite{hoffman2014no}. This can involve adaptive techniques that adjust proposal distributions during the sampling process based on the behavior of the Markov chain, thereby optimizing the balance between exploration and exploitation of the sample space.

In recent years, normalizing flow is used to MCMC sampling. A normalizing flow (NF) is an invertible map $T : \Omega \to \Omega$ that is optimized to transport samples from a base measure $$\nu_B(dx) = \rho_B(x) \, dx$$ to a given target distribution. The goal is to produce a map $T^*$ with inverse $\overline{T}^*$ such that an expectation of an observable with respect to $\rho^*$ can be estimated by transforming samples from the base density to the target, i.e., if $x_B$ is drawn from $\rho_B(x)$ then $T^*(x_B)$ is a sample from $\rho^*(x)$ so that for any suitable observable $O$ we have
$$
\int_\Omega O(T^*(x))\rho_B(x)\, dx = \int_\Omega O(x)\rho^*(x)\, dx. 
$$
The existence of such a map $T^*$ is guaranteed under general conditions on $\rho^*$ and $\rho_B$ investigated e.g., in the context of optimal transport theory \cite{Villani2003,Santambrogio2015}. In actual operations, it is only necessary to obtain an approximation of $T^*: T$ \cite{Andral2024CombiningNF,Gabrie2021AdaptiveMC},
$$
\hat{\rho}(x) = \rho_B(\overline{T}(x)) \left| \det(J_{\overline{T}}(x)) \right|
$$ where $J_{\overline{T}}$ is the Jacobian of the inverse map $\overline{T}$. 

Overall, while MCMC remains a powerful method and these advancements seek to mitigate the slow convergence and poor scalability issues associated with high-dimensional problems, facilitating more efficient and robust sampling from complex distribution landscapes, different MCMC algorithms (e.g., Metropolis-Hastings, Gibbs sampling, Hamiltonian Monte Carlo) \cite{gilks1996markov} can face distinct challenges in practice, which we will outline below as key theoretical and practical challenges commonly encountered by MCMC methods.



\subsection{Challenges in MCMC Methods}

Despite the widespread application of MCMC methods, several key challenges hinder their efficiency, particularly in high-dimensional or complex distribution spaces.

\subsubsection{Slow Convergence and High Autocorrelation}

Traditional MCMC methods, such as Metropolis-Hastings and Gibbs sampling, rely on random walk dynamics to explore the solution space. However, in high-dimensional settings, the Markov chain often exhibits high autocorrelation between successive samples, requiring a long burn-in period to achieve convergence.

The convergence rate is largely governed by the spectral gap of the transition kernel. Let $P$ denote the transition matrix of the Markov chain, and $\lambda_2$ the second-largest eigenvalue of $P$. The mixing time $\tau_{\text{mix}}$, which quantifies the time required for the chain to approximate the target distribution, scales approximately as:
\[
\tau_{\text{mix}} \propto \frac{1}{1 - \lambda_2}.
\]
In high-dimensional spaces, $\lambda_2$ approaches 1, resulting in a slow mixing process that limits sampling efficiency \citep{robert2004monte}.

\subsubsection{Curse of Dimensionality}

As the number of dimensions increases, the volume of the sample space grows exponentially. This phenomenon, known as the "curse of dimensionality," poses significant challenges for MCMC methods. Random walk-based algorithms struggle to explore the entire space effectively, often requiring a prohibitively large number of iterations to identify high-density regions of the target distribution.

\subsubsection{Proposal Distribution Tuning}

The efficiency of MCMC methods depends critically on the choice of the proposal distribution. A poorly designed proposal distribution can lead to low acceptance rates and inefficient sampling. Adaptive MCMC methods attempt to address this by dynamically adjusting the proposal distribution, but these approaches often require careful tuning and are computationally expensive.


\section{Research Objectives}
The main objectives of this study are as follows:
\begin{itemize}
    \item Develop a framework that integrates deep learning with Monte Carlo methods to improve sampling efficiency in high-dimensional and complex spaces.
    \item Design a neural network-based strategy to guide the Monte Carlo sampling process, potentially learning the structure of the target distribution or the dynamics of complex systems.
    \item Apply the proposed LgMC framework to various application areas, including:
    \begin{itemize}
        \item Bayesian inference for high-dimensional parameter estimation.
        \item Probabilistic graphical models (such as Bayesian networks).
        \item Optimization of exploration-exploitation trade-offs in reinforcement learning in high-dimensional spaces.
    \end{itemize}
    \item Evaluate the effectiveness and scalability of the proposed method on synthetic and real-world datasets, comparing its performance to traditional Monte Carlo methods and state-of-the-art techniques, with a focus on computational efficiency, accuracy, and convergence speed.
\end{itemize}

\section{Research Questions}
This study will address the following key questions:
\begin{itemize}
    \item How can deep learning techniques be effectively used to guide the Monte Carlo sampling process?
    \item Which neural network architectures are most suitable for learning optimal sampling strategies in high-dimensional and complex problem spaces?
    \item Can the LgMC method significantly reduce computational costs and improve convergence speed in high-dimensional and complex inference tasks, compared to traditional Monte Carlo and MCMC methods?
    \item How can the learned sampling strategy be generalized across different domains and tasks, ensuring its robustness and scalability?
\end{itemize}






\section{Methodology}

%由于我们是research proposal, 目前我只有初步的想法.接下来我通过一个例子把这个想法的核心表述出来
Since this is a research proposal and I currently have only an initial idea, I will illustrate the core concept of the LgMC method using the Metropolis-Hastings algorithm as an example, and will provide further improvements and examples in subsequent research.

\subsection{Review: Metropolis-Hastings Algorithm}

The Metropolis-Hastings algorithm is one of the most commonly used MCMC methods. Its core idea is to construct a Markov chain whose stationary distribution is the target distribution $\pi(x)$. The Metropolis-Hastings algorithm can be described using the following formulas:

Given the current state $x_t$, a new candidate sample $x'$ is proposed to replace $x_t$:

\[
x' \sim q(x'|x_t)
\]

where $q(x'|x_t)$ is the proposal distribution.

The probability of accepting the candidate sample $x'$ is given by:

\[
\alpha(x_t, x') = \min \left(1, \frac{\pi(x') q(x_t|x')}{\pi(x_t) q(x'|x_t)} \right)
\]

If $x'$ is accepted, the state is updated as $x_{t+1} = x'$; otherwise, the state remains $x_{t+1} = x_t$.

This process is iterated to generate a new sequence of samples.


While Metropolis-Hastings is a powerful tool, it does have some limitations such as:

\begin{itemize}
    \item \textbf{Slow Convergence:} MCMC algorithms, especially in high-dimensional spaces, often have long burn-in periods and slow convergence rates.
    \item \textbf{Local Optima Issue:} In multimodal distributions, traditional MCMC methods may get stuck in local modes, leading to inefficient sampling.
\end{itemize}



\subsection{Learning-guided Monte Carlo Methods (LgMC)}
To address these challenges, we propose the \textbf{Learning-guided Monte Carlo (LgMC)} method, which leverages deep learning to learn an optimal sampling strategy. The idea is to use a neural network to guide the sampling process by learning the structure of the target distribution and guide the sampling chain, reducing sampling from low-probability regions.

\subsubsection{Guidance Function and Strategy}
Given the target distribution $\pi(x)$, we aim to use a neural network $f(x, \theta)$ to learn a guidance strategy such that the sampling process can explore the high-density regions of the target distribution more efficiently.

Target Distribution and Learning Task: We use the neural network $f(x, \theta)$ to fit the target distribution $\pi(x)$ and optimize the network parameters $\theta$ :

\[
\theta^* = \arg\min_\theta \mathbb{E}_{x \sim \pi(x)} \left[ (f(x, \theta) - \log \pi(x))^2 \right]
\]

The optimization objective is to minimize the error between the neural network's predicted probability density and the true target distribution.

\subsubsection{Correction of Proposal Distribution}
Through the learned guidance function, the LgMC method improves the traditional proposal distribution $q(x'|x_t)$. In the traditional Metropolis-Hastings algorithm, the proposal distribution is often a simple symmetric distribution (e.g., a Gaussian distribution), while LgMC uses a neural network model to learn and guide the sampling process. Assuming that the network $f(x, \theta)$ learns to capture the structure of the target distribution, the new proposal distribution is adjusted as follows:

\[
q_{\text{guided}}(x'|x_t) \propto f(x', \theta) q(x'|x_t)
\]

With the guidance function $f(x, \theta)$, the proposal distribution is no longer a simple symmetric distribution, but adapts based on the shape of the target distribution.

\subsubsection{New Sampling Strategy}
Through this method, the mathematical form of the sampling process in LgMC is similar to an enhanced Metropolis-Hastings method, where the proposal distribution, based on the traditional symmetric proposal distribution, is corrected by a guidance distribution learned by the neural network.

\[
\alpha(x_t, x') = \min \left( 1, \frac{\pi(x_t) f(x', \theta)}{\pi(x') f(x_t, \theta)} \cdot \frac{q(x_t|x')}{q(x'|x_t)} \right)
\]

Here, $f(x, \theta)$ is the guidance function learned through deep learning, which provides more efficient guidance to the sampling chain, thus improving the efficiency of the sampling process.


\begin{algorithm}[H]
\caption{Learning-guided Monte Carlo (LgMC)}
\begin{algorithmic}[1]
\State \textbf{Input:} Target distribution \(\pi(x)\), initial state \(x_0\), proposal distribution \(q(x'|x_t)\), learning model \(f(x, \theta)\), number of iterations \(T\), threshold $N$
\State \textbf{Output:} Sequence \(\{x_{N+1}, x_{N+2}, \dots, x_T\}\)

\State \textbf{Initialize:}
\State \quad Set \(x_0\) as the starting point.
\State \quad Initialize learning model parameters \(\theta\).

\State \textbf{Training the learning model:}
\For{each sample \(x \sim \pi(x)\)}
    \State Optimize the learning model to minimize the error:
    \[
    \theta^* = \arg\min_\theta \mathbb{E}_{x \sim \pi(x)} \left[ (f(x, \theta) - \log \pi(x))^2 \right]
    \]
    \State This step aims to learn the structure of the target distribution.
\EndFor

\State \textbf{Sampling Process (Iteration \(t = 1 \dots T\)):}
\For{each iteration \(t\)}
    \State Propose a new candidate sample \(x' \sim q(x'|x_t)\).
    \State Compute the acceptance probability \(\alpha(x_t, x')\) using the guidance function \(f(x, \theta)\):
    \[
    \alpha(x_t, x') = \min \left( 1, \frac{\pi(x') f(x', \theta)}{\pi(x_t) f(x_t, \theta)} \cdot \frac{q(x_t|x')}{q(x'|x_t)} \right)
    \]
    \State Accept the candidate \(x'\) with probability \(\alpha(x_t, x')\):
    \If{accepted}
        \State \(x_{t+1} = x'\)
    \Else
        \State \(x_{t+1} = x_t\)
    \EndIf
\EndFor

\State \textbf{Return} the sequence of samples \(\{x_{N+1}, x_{N+2}, \dots, x_T\}\).
\end{algorithmic}
\end{algorithm}


%\subsubsection{Convergence and Error Analysis in Mathematics}

\subsubsection{Convergence}
Although the LgMC algorithm incorporates a learning guidance, it still needs to be proven that the guidance algorithm can maintain the stationary distribution as the target distribution $\pi(x)$. Specifically, the following conditions need to be verified:
\begin{itemize}
    \item After enough iterations, the sampling chain will reach the target distribution $\pi(x)$, i.e., the Markov chain is ergodic.
    \item Invariance: The acceptance rate, after proper adjustment, still guarantees that the target distribution is the stationary distribution.
\end{itemize}

\subsubsection{Error Bound}
For the LgMC algorithm, it is necessary to analyze the error that may be introduced during the network learning process. Suppose the neural network has an error $\epsilon(\theta)$ in learning the target distribution $\pi(x)$, then the bias in the guidance distribution during the sampling process will affect the quality of the sampling.

In some simple cases, we can estimate the size of this error using methods like Taylor expansion. Suppose the neural network's output function $f(x, \theta)$ can effectively approximate the logarithmic density function of the target distribution $\log \pi(x)$, then the impact of the error will be sublinear (i.e., as the number of network training iterations increases, the error will decrease).

\subsection{Summary}
By combining MCMC with deep learning, LgMC will not only addresses the local optima and slow convergence issues in traditional MCMC methods but also guarantees the validity of the sampling through mathematical theory.  We will provide LgMC more theoretical support. In future research, we plan to try more mathematical tools, such as variational inference and optimal transport, to further improve the efficiency and accuracy of the sampling process.

%And in the future, we will make learning-based improvements to more MCMC algorithms, such as Gibbs sampling and HMC, as well as provide theoretical support for the LgMC algorithms.
In the future, we will make learning-based improvements to more MCMC algorithms, such as Gibbs sampling and HMC, as well as provide theoretical support for the LgMC algorithms. The scope will not be limited to MCMC, but also includes other things like Sequential Monte Carlo (SMC) methods.

%Additionally, we will explore enhancements not only for MCMC but also for Sequential Monte Carlo (SMC) methods.



\bibliographystyle{apalike}
%\bibliographystyle{plainnat}
\bibliography{ref}


\section{Appendix}

\subsection{Research Projects}

\subsubsection{Supervised Fine-Tuning of GPT-2 for Conversational Modeling}
\textit{Chinese University of Hong Kong, Shenzhen} \\
2023.10 - 2023.12 \\
%We compare convergence rates and GPU memory consumption across various optimizers, including SGD, SGD with momentum, Nesterov accelerated SGD, and Adam/AdamW. Identify the most effective optimization strategy for improving dialogue generation efficiency and performance in GPT-2.

\begin{itemize}
    \item \textbf{Supervised Fine-Tuning}: Supervised Fine-Tuning  was applied by training the model on a labeled conversational dataset to optimize its performance in dialogue generation. This approach improved the model’s ability to understand user inputs and enhance response quality.

    \item \textbf{Optimizer Comparison}: The performance of various optimizers was compared, including standard SGD, SGD with momentum, Nesterov accelerated SGD, and Adam/AdamW. The focus was on evaluating the convergence speed, training efficiency, and GPU memory consumption of each optimizer, in order to select the most suitable optimization method for this task.

    \item \textbf{Model Performance Evaluation}: The fine-tuned GPT-2 model demonstrated significant improvements in context understanding, coherent dialogue generation, and maintaining dialogue relevance. Compared to the baseline un-tuned model, the optimized version showed better natural language understanding and generation ability in multi-turn conversations.
\end{itemize}



\subsubsection{Valuation and Multi-Factor Strategy of Chinese Convertible Bonds}
%\textit{Chinese University of Hong Kong, Shenzhen} \\
\textit{Advisor: Gongqiu Zhang} \\
2023.3 - Present \\

\begin{itemize}
    \item \textbf{Valuation of Chinese Convertible Bonds}: Stock price paths are simulated using the Monte Carlo method, and the least squares regression framework is applied to backtrack from the maturity date $T$ to derive the price of convertible bonds.
    \item \textbf{Combination of Machine Learning Regression Methods}: Various machine learning regression methods are combined, with training and testing datasets partitioned using a sliding window approach. Through backtesting, we identified effective factor combinations and the best regression method, forming the optimal multi-factor strategy for Chinese convertible bonds.
    \item \textbf{Real-time Automated Trading}: By implementing a web scraper on JiSiLu (a real-time convertible bond data platform), we automated daily trading for convertible bonds.
    \item \textbf{Development of a Quantitative Trading Platform}: In the future, we plan to develop our own quantitative trading platform for automated analysis and trading of convertible bonds.
\end{itemize}






\subsubsection{Image Restoration Method Based on Wavelet Framework}
\textit{Advisor: Jae Kyu Choi} \\
2020.03 - 2020.07 \\
%Led the research on three reconstruction models based on wavelet framework, focusing on the resolution methods and mathematical significance of typical models. Proposed a new algorithm based on the Alternating Direction Method of Multipliers (ADMM) and proved its convergence. Conducted numerical experiments using MATLAB to validate the algorithm's effectiveness in image denoising, reconstruction, and restoration. Out of 60 participants, only 10 qualified for the project defense, achieving a satisfactory evaluation, demonstrating a deep understanding of the project and research outcomes.
\begin{itemize}
    \item We investigated the significance and interrelationships among three reconstruction models based on the wavelet framework, with a focus on solving one of the most representative models.
    \item We proposed a solution algorithm based on the Alternating Direction Method of Multipliers (ADMM) and provided a convergence proof.
    \item Numerical experiments were conducted in Matlab, including applications such as image denoising, image reconstruction, and image inpainting.
\end{itemize}

\subsection{Publications and Preprints}
Yu Liu and Gongqiu Zhang. \\
\textit{Valuation Model of Chinese Convertible Bonds Based on Monte Carlo Simulation}, 2024; arXiv:2409.06496.



\end{document}