<head>  
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  
</head>

# 关于蒙特卡洛的核心

引文：提到随机，都会想到用蒙特卡洛方法，并且经常用。
但是这里面有两个最核心问题研究至今。

### a. **分布问题**

随机生成之前，得告诉我，要我生成的东西是有什么规律，统计学上称之为分布。很多时候我们想从一个复杂的分布中采样，但这个分布我们可能**不知道解析式**，或者知道但**难以直接采样**。一般来讲，想要自动生成样本，是需要知道具体的表达式的，下面是两种常见的解决方式。

* **重要性采样**解决的是“我不能直接从目标分布采样，那我能不能从一个简单的分布采样后再做加权？”
* **MCMC（马尔可夫链蒙特卡洛）**就是通过构造一个马尔可夫链，让它**在长时间运行后稳定地分布在目标分布上**。

### b. **生成效率问题**

即便我们知道怎么采样，也未必采得好。比如，

* 样本可能都集中在目标分布低概率的地方（低效）；
* 有些重要区域没采到（偏差大）；
* 或者样本之间相关性太强（信息冗余）。

**“分布难建、样本难采”是蒙特卡洛方法的核心挑战，也是其在21世纪仍然是热门研究话题的根本原因。**

## 1. 基础蒙特卡洛方法

蒙特卡洛方法（Monte Carlo Method）最早由冯·诺依曼等人提出，最初用于核武器研究。其核心思想是利用**随机采样**来求解具有确定性解的问题。

### 1.1 基本原理

以数值积分为例，设我们希望估计以下定积分：

$$
I = \int_a^b f(x)dx
$$

基本步骤如下：

1. 从区间 [a,b] 上的均匀分布中抽取 $N$ 个样本：$x_1, x_2, \ldots, x_N$
2. 构造估计值：

$$
\hat{I} = \frac{b-a}{N} \sum_{i=1}^N f(x_i)
$$

这种估计的误差收敛速度为 $O(1/\sqrt{N})$，即：

$$
\mathbb{E}[(\hat{I} - I)^2] \propto \frac{1}{N}
$$

### 1.2 误差分析
根据中心极限定理，当样本量 $N$ 足够大时，估计量 $\hat{I}$ 近似服从正态分布：

$$
\sqrt{N}(\hat{I} - I) \xrightarrow{d} N(0, \sigma^2)
$$

其中方差 $\sigma^2$ 可以通过如下方式计算：

$$
\sigma^2 = (b-a)^2 \cdot \text{Var}[f(X)] = (b-a)^2 \left(\mathbb{E}[f(X)^2] - \mathbb{E}[f(X)]^2\right)
$$

其中 $X$ 是区间 $[a,b]$ 上的均匀分布随机变量。这就解释了为什么误差以 $O(1/\sqrt{N})$ 的速度收敛。

具体来说，当我们增加样本量时：
1. 估计量的标准差减小为原来的 $1/\sqrt{N}$
2. 置信区间宽度也相应减小
3. 但收敛速度相对较慢，需要增加4倍样本才能将误差减半

---

## 2. 重要性采样（Importance Sampling）

### 2.1 基本思想

重要性采样通过引入一个**更容易采样**的建议分布 \$q(x)\$，以减少方差并提高估计效率。基本思路是将期望转换为：

$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x)dx = \int f(x) \frac{p(x)}{q(x)} q(x)dx
$$

从而可以用以下无偏估计器逼近：

$$
\frac{1}{N} \sum_{i=1}^N f(x_i)\frac{p(x_i)}{q(x_i)}, \quad x_i \sim q(x)
$$

### 2.1.1 最优建议分布的推导
为了找到最优的建议分布，我们需要最小化估计量的方差。推导过程如下：

1. 首先写出方差表达式：
$$
\text{Var}_{q}[w(X)f(X)] = \mathbb{E}_{q}\left[\left(w(X)f(X)\right)^2\right] - \left(\mathbb{E}_{p}[f(X)]\right)^2
$$

其中，$w(x) = p(x)/q(x)$ 是重要性权重。

2. 将第一项展开：
$$
\mathbb{E}_{q}\left[\left(w(X)f(X)\right)^2\right] = \int \frac{p(x)^2}{q(x)^2}f(x)^2q(x)dx = \int \frac{p(x)^2f(x)^2}{q(x)}dx
$$

3. 使用变分法最小化方差，可以得到：
$$
q^*(x) \propto |f(x)|p(x)
$$

这表明最优建议分布应该与目标分布和被积函数的乘积成比例。

### 2.1.2 理想建议分布条件详解

1. **采样容易**
   - 数学要求：建议分布 $q(x)$ 应该属于常见分布族（如高斯分布、t分布等）
   - 原因：需要能够直接使用计算机生成随机数
   - 例子：如果使用多维高斯分布 $N(\mu, \Sigma)$，应该能高效生成样本

2. **覆盖目标分布**
   - 数学表述：对任意 $x$，若 $f(x)p(x) \neq 0$，则 $q(x) > 0$
   - 重要性：避免估计有偏
   - 反例：如果某个区域 $A$ 上 $q(x) = 0$ 但 $f(x)p(x) \neq 0$，则：
     $$
     \int_A f(x)p(x)dx \neq 0 \text{ 但永远无法通过采样获得}
     $$

3. **尾部较重**
   - 数学描述：对于任意 $x$，比值 $\frac{p(x)}{q(x)}$ 应该有界
   - 原因：防止权重 $w(x) = \frac{p(x)}{q(x)}$ 方差无穷大
   - 理论界限：要求 $\mathbb{E}_q[w(X)^2] < \infty$

4. **形状相似**
   - 数学要求：$q(x)$ 应该近似于 $|f(x)|p(x)$
   - 理论基础：来自最优建议分布的推导
   - 实践指导：可以通过pilot runs估计目标分布的形状

### 2.1.3 常见问题与解决策略

**1. 权重退化**

* 现象：少数样本占据主导
* 解决：计算有效样本量（ESS）进行监控：

  $$
  ESS = \frac{(\sum_{i=1}^N w_i)^2}{\sum_{i=1}^N w_i^2}
  $$

**2. 自适应策略**

动态更新建议分布：

$$
q_t(x) = (1 - \alpha) q_{t-1}(x) + \alpha \hat{p}_t(x)
$$

### 2.2 与MCMC的比较

| 方法    | 样本依赖性 | 优势        | 劣势       |
| ----- | ----- | --------- | -------- |
| 重要性采样 | 独立样本  | 适用于高效建议分布 | 建议分布难以设计 |
| MCMC  | 相关样本  | 可应对复杂分布   | 可能收敛慢    |

---

## 3. 马尔可夫链蒙特卡洛（MCMC）

### 3.1 基本思想

MCMC通过构造一个以目标分布为**平稳分布**的马尔可夫链，从而采样复杂的分布，尤其适用于高维场景。

### 3.2 Metropolis-Hastings 算法

Metropolis-Hastings算法是MCMC的核心方法之一，其目标是通过构造一个马尔可夫链，使其平稳分布为目标分布 $\pi(x)$。

#### 3.2.1 算法步骤

1. 当前状态为 $x_t$，从建议分布 $q(x'|x_t)$ 中生成候选样本 $x'$。
2. 计算接受概率：
   $$
   \alpha(x_t, x') = \min\left(1, \frac{\pi(x') q(x_t|x')}{\pi(x_t) q(x'|x_t)}\right)
   $$
3. 以概率 $\alpha(x_t, x')$ 接受候选样本 $x'$：
   - 如果接受，则令 $x_{t+1} = x'$；
   - 否则，令 $x_{t+1} = x_t$。

#### 3.2.2 算法推导

Metropolis-Hastings算法的推导基于马尔可夫链的平稳性条件，即目标分布 $\pi(x)$ 满足详细平衡条件：
$$
\pi(x)P(x \to x') = \pi(x')P(x' \to x)
$$

其中，$P(x \to x')$ 是从状态 $x$ 转移到状态 $x'$ 的概率。将其分解为两部分：
$$
P(x \to x') = q(x'|x) \cdot A(x \to x')
$$
- $q(x'|x)$ 是建议分布，表示从 $x$ 生成 $x'$ 的概率；
- $A(x \to x')$ 是接受概率。

将其代入详细平衡条件：
$$
\pi(x)q(x'|x)A(x \to x') = \pi(x')q(x|x')A(x' \to x)
$$

为了满足详细平衡条件，可以选择如下形式的接受概率：
$$
A(x \to x') = \min\left(1, \frac{\pi(x')q(x|x')}{\pi(x)q(x'|x)}\right)
$$

这样，Metropolis-Hastings算法保证了马尔可夫链的平稳分布为目标分布 $\pi(x)$。

#### 3.2.3 算法特点

1. **灵活性**：可以选择任意形式的建议分布 $q(x'|x)$。
2. **适用性**：适用于目标分布 $\pi(x)$ 的归一化常数未知的情况。
3. **效率**：接受率依赖于建议分布的选择，建议分布越接近目标分布，采样效率越高。

### 3.3 常见问题

1. **收敛慢**：预热期长，尤其在高维空间中。
2. **局部最优**：在多模态分布下易陷入局部区域。

---

## 4. MCMC方法的改进

### 4.1 Gibbs采样

适用于条件概率分布容易求取的场景：

$$
x_i^{(t+1)} \sim p(x_i | x_{-i}^{(t)})
$$

### 4.2 Hamiltonian Monte Carlo（HMC）

引入动量变量，模拟物理系统：

$$
H(x, p) = -\log \pi(x) + \frac{1}{2}p^T p
$$

通过哈密顿动力学获得高效采样：

$$
\begin{cases}
\frac{dx}{dt} = p \\
\frac{dp}{dt} = -\nabla U(x)
\end{cases}
$$

### 4.3 自适应MCMC

不断调整建议分布参数：

$$
q_{\theta_n}(x'|x_n) \rightarrow \text{最优参数}
$$

---

## 5. 神经网络辅助的MCMC

### 5.1 深度学习在MCMC中的应用

主要两个方向：

1. 学习建议分布
2. 学习目标分布结构特征

### 5.2 学习引导的蒙特卡洛（LgMC）

核心思想：使用神经网络引导采样链更高效地探索目标分布。

#### 5.2.1 引导函数与策略

设神经网络 \$f(x, \theta)\$ 拟合 \$\log \pi(x)\$，优化目标：

$$
\theta^* = \arg\min_\theta \mathbb{E}_{x \sim \pi(x)} \left[(f(x, \theta) - \log \pi(x))^2\right]
$$

#### 5.2.2 建议分布修正

通过引导函数修正传统建议分布：

$$
q_{\text{guided}}(x'|x_t) \propto f(x', \theta) q(x'|x_t)
$$

#### 5.2.3 新的采样策略

增强版的 Metropolis-Hastings 接受率：

$$
\alpha(x_t, x') = \min \left(1, \frac{\pi(x_t) f(x', \theta)}{\pi(x') f(x_t, \theta)} \cdot \frac{q(x_t|x')}{q(x'|x_t)} \right)
$$

---

### 5.3 理论分析

#### 5.3.1 收敛性

LgMC 所构建的马尔可夫链仍保持 \$\pi(x)\$ 为平稳分布，满足遍历性。

#### 5.3.2 误差界限

神经网络带来的误差可控。随着训练迭代增多，误差趋于减小。

#### 5.3.3 计算复杂度

每次迭代复杂度约为：

$$
O(P + Q)
$$

其中 \$P\$ 是神经网络参数量，\$Q\$ 是样本数量。

---

### 5.4 总结

LgMC 方法结合了深度学习与传统 MCMC 的优点，不仅缓解了收敛慢与局部最优的问题，还在理论上保持了准确性与稳定性。未来的发展方向包括更高效的网络结构、更快的训练机制和更强的泛化能力。

---
